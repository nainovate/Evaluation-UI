{
  "evaluationSession": {
    "id": "eval_1752658961978_9v46ah7so",
    "createdAt": "2025-07-16T09:42:41.978Z",
    "lastModified": "2025-07-18T06:15:28.031Z",
    "status": "dataset_selected"
  },
  "dataset": {
    "uid": "eval_dataset_001",
    "id": "eval_dataset_001",
    "name": "Customer Support QA Evaluation",
    "selectedAt": "2025-07-18T05:58:27.543Z",
    "taskType": "Question Answering",
    "rows": 500,
    "columns": [
      "question",
      "expected_answer",
      "context",
      "category"
    ]
  },
  "deployment": {
    "id": "2",
    "name": "staging-v2",
    "model": "claude-3-haiku-20240307",
    "provider": "Anthropic",
    "selectedAt": "2025-07-18T06:04:59.281Z"
  },
  "metrics": {
    "categories": [
      {
        "id": "answer-quality",
        "name": "Answer Quality",
        "description": "Evaluate how well responses answer questions and stay grounded in context",
        "icon": "üéØ",
        "color": "indigo",
        "selected": false,
        "subMetrics": [
          {
            "id": "answer-relevancy",
            "name": "Answer Relevancy",
            "description": "Measures how relevant the answer is to the given question",
            "enabled": false
          },
          {
            "id": "faithfulness",
            "name": "Faithfulness",
            "description": "Evaluates if the answer is grounded in the provided context",
            "enabled": false
          },
          {
            "id": "hallucination-detection",
            "name": "Hallucination Detection",
            "description": "Identifies when the model generates false or unsupported information",
            "enabled": false
          }
        ]
      },
      {
        "id": "context-understanding",
        "name": "Context Understanding",
        "description": "Measure how well models comprehend and utilize provided context",
        "icon": "üìä",
        "color": "green",
        "selected": true,
        "subMetrics": [
          {
            "id": "context-recall",
            "name": "Context Recall",
            "description": "Measures how well the model recalls information from the context",
            "enabled": false
          },
          {
            "id": "factual-consistency",
            "name": "Factual Consistency",
            "description": "Evaluates consistency of facts between context and answer",
            "enabled": true
          }
        ]
      },
      {
        "id": "similarity-accuracy",
        "name": "Similarity & Accuracy",
        "description": "Compare outputs against expected responses using similarity measures",
        "icon": "‚öñÔ∏è",
        "color": "yellow",
        "selected": false,
        "subMetrics": [
          {
            "id": "exact-match",
            "name": "Exact Match",
            "description": "Checks if the answer exactly matches the expected response",
            "enabled": false
          },
          {
            "id": "bert-score",
            "name": "BERTScore",
            "description": "Semantic similarity using BERT embeddings",
            "enabled": false
          },
          {
            "id": "embedding-distance",
            "name": "Embedding Distance",
            "description": "Cosine similarity between answer and reference embeddings",
            "enabled": false
          }
        ]
      },
      {
        "id": "language-quality",
        "name": "Language Quality",
        "description": "Assess linguistic quality, fluency, and coherence of generated text",
        "icon": "üìù",
        "color": "purple",
        "selected": false,
        "subMetrics": [
          {
            "id": "fluency",
            "name": "Fluency",
            "description": "Evaluates the natural flow and readability of the text",
            "enabled": false
          },
          {
            "id": "coherence",
            "name": "Coherence",
            "description": "Measures logical consistency and clarity of the response",
            "enabled": false
          },
          {
            "id": "conciseness",
            "name": "Conciseness",
            "description": "Assesses if the answer is appropriately brief and to the point",
            "enabled": false
          }
        ]
      }
    ],
    "selectedCategory": "context-understanding",
    "totalSelected": 1,
    "configuration": {
      "evaluationModel": "gpt-4",
      "batchSize": 50,
      "timeout": 30
    },
    "configuredAt": "2025-07-18T06:15:27.966Z"
  },
  "execution": {
    "startedAt": null,
    "completedAt": null,
    "status": null,
    "results": null
  }
}