[
  {
    "id": "eval_dataset_001",
    "uid": "eval_dataset_001",
    "name": "Customer Support QA Evaluation",
    "description": "Evaluation dataset for customer support chatbot performance with question-answer pairs",
    "size": 1024000,
    "status": "valid",
    "columns": ["question", "expected_answer", "context", "category"],
    "rows": 500,
    "uploadedAt": "2024-01-15T10:30:00Z",
    "filePath": "./data/customer_support_qa_eval.csv",
    "originalFileName": "customer_support_qa_eval.csv",
    "taskType": "Question Answering",
    "tags": ["customer-support", "factual-qa", "evaluation"],
    "format": "CSV"
  },
  {
    "id": "eval_dataset_002",
    "uid": "eval_dataset_002", 
    "name": "Text Summarization Benchmark",
    "description": "Standard benchmark for evaluating text summarization models with reference summaries",
    "size": 2048000,
    "status": "valid",
    "columns": ["input_text", "reference_summary", "length_constraint", "domain"],
    "rows": 1000,
    "uploadedAt": "2024-01-10T14:20:00Z",
    "filePath": "./data/text_summarization_benchmark.csv",
    "originalFileName": "text_summarization_benchmark.csv",
    "taskType": "Summarization",
    "tags": ["summarization", "abstractive", "evaluation"],
    "format": "CSV"
  },
  {
    "id": "eval_dataset_003",
    "uid": "eval_dataset_003",
    "name": "Incomplete Evaluation Set",
    "description": "Missing reference outputs for proper evaluation - needs to be fixed",
    "size": 512000,
    "status": "invalid",
    "columns": ["input_text"],
    "rows": 200,
    "uploadedAt": "2024-01-08T09:15:00Z",
    "filePath": "./data/incomplete_eval_set.csv",
    "originalFileName": "incomplete_eval_set.csv",
    "taskType": "Classification",
    "tags": ["incomplete", "invalid"],
    "format": "CSV"
  },
  
  {
    "id": "eval_dataset_005",
    "uid": "eval_dataset_005",
    "name": "Multi-turn Conversation QA",
    "description": "Conversational QA with context awareness and multi-turn dialogue evaluation",
    "size": 1536000,
    "status": "valid",
    "columns": ["conversation_history", "question", "expected_answer", "context"],
    "rows": 800,
    "uploadedAt": "2024-01-14T11:20:00Z",
    "filePath": "./data/conversation_qa_eval.csv",
    "originalFileName": "conversation_qa_eval.csv",
    "taskType": "Conversational QA",
    "tags": ["multi-turn", "dialogue-qa", "evaluation"],
    "format": "CSV"
  },
  {
    "id": "eval_dataset_006",
    "uid": "eval_dataset_006",
    "name": "Document Retrieval Benchmark",
    "description": "Information retrieval evaluation with passage ranking and semantic search",
    "size": 2560000,
    "status": "valid",
    "columns": ["query", "relevant_documents", "irrelevant_documents", "relevance_scores"],
    "rows": 300,
    "uploadedAt": "2024-01-16T09:30:00Z",
    "filePath": "./data/document_retrieval_benchmark.csv",
    "originalFileName": "document_retrieval_benchmark.csv",
    "taskType": "Retrieval",
    "tags": ["document-retrieval", "semantic-search", "evaluation"],
    "format": "CSV"
  }
]